{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Multi-digit Recognition on Multi-MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class MultiDigitMNISTDataset(Dataset):\n",
    "  def __init__(self, root_dir, transform=None):\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform\n",
    "    self.folder_names = sorted(os.listdir(root_dir))\n",
    "    self.image_paths = []\n",
    "    self.labels = []\n",
    "\n",
    "    for folder_name in self.folder_names:\n",
    "      folder_path = os.path.join(root_dir, folder_name)\n",
    "      if os.path.isdir(folder_path):\n",
    "        for image_name in os.listdir(folder_path):\n",
    "          image_path = os.path.join(folder_path, image_name)\n",
    "          self.image_paths.append(image_path)\n",
    "          self.labels.append(folder_name)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_paths)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image_path = self.image_paths[idx]\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    label = self.labels[idx]\n",
    "\n",
    "    if self.transform:\n",
    "      image = self.transform(image)\n",
    "\n",
    "    tens_digit = int(label[0])\n",
    "    units_digit = int(label[1])\n",
    "\n",
    "    return image, (tens_digit, units_digit)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "root = 'double_mnist_seed_123_image_size_64_64'\n",
    "\n",
    "train_dataset = MultiDigitMNISTDataset(root_dir=root+'/train', transform=transform)\n",
    "val_dataset = MultiDigitMNISTDataset(root_dir=root+'/val', transform=transform)\n",
    "test_dataset = MultiDigitMNISTDataset(root_dir=root+'/test', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1.1: MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "  def __init__(self, shape):\n",
    "    super(Reshape, self).__init__()\n",
    "    self.shape = shape\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x.view(self.shape)\n",
    "\n",
    "class MultiDigitMLP(nn.Module):\n",
    "  def __init__(self, input_size=2048, hidden_size=128, dropout=0.2, learning_rate=0.001, batch_size=64, num_epochs=10):\n",
    "    super(MultiDigitMLP, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    self.hidden_layer = nn.Sequential(\n",
    "      nn.Linear(input_size, hidden_size),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "    self.output = nn.Sequential(\n",
    "      nn.Linear(hidden_size, 10)\n",
    "    )\n",
    "\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    self.num_epochs = num_epochs\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.reshape(x.size(0), -1)\n",
    "    hidden_output = self.hidden_layer(x)\n",
    "    outputs = self.output(hidden_output)\n",
    "    return outputs\n",
    "\n",
    "  def train_model(self, train_data, val_data):\n",
    "    train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(self.num_epochs):\n",
    "      self.train()\n",
    "      train_loss = 0.0\n",
    "      correct = 0\n",
    "      total = 0\n",
    "\n",
    "      val_loss = 0.0\n",
    "      val_correct = 0\n",
    "      val_total = 0\n",
    "\n",
    "      for inputs, (labels_tens, labels_ones) in train_loader:\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs_tens = self(inputs[:, :, :, :32])\n",
    "        outputs_ones = self(inputs[:, :, :, 32:])\n",
    "        losses_tens = self.criterion(outputs_tens, labels_tens)\n",
    "        losses_ones = self.criterion(outputs_ones, labels_ones)\n",
    "        total_loss = losses_tens + losses_ones\n",
    "        total_loss.backward()\n",
    "\n",
    "        train_loss += total_loss.item()\n",
    "        _, predicted_ones = torch.max(outputs_ones.data, 1)\n",
    "        _, predicted_tens = torch.max(outputs_tens.data, 1)\n",
    "        total += labels_ones.size(0)\n",
    "        correct += (torch.logical_and(predicted_ones == labels_ones, predicted_tens == labels_tens)).sum().item()\n",
    "\n",
    "      self.eval()\n",
    "      for inputs, (labels_tens, labels_ones) in val_loader:\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs_tens, outputs_ones = self(inputs[:, :, :, :32]), self(inputs[:, :, :, 32:])\n",
    "        losses_tens = self.criterion(outputs_tens, labels_tens)\n",
    "        losses_ones = self.criterion(outputs_ones, labels_ones)\n",
    "        total_loss = losses_tens + losses_ones\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        val_loss += total_loss.item()\n",
    "        _, predicted_ones = torch.max(outputs_ones.data, 1)\n",
    "        _, predicted_tens = torch.max(outputs_tens.data, 1)\n",
    "        val_total += labels_ones.size(0)\n",
    "        val_correct += (torch.logical_and(predicted_ones == labels_ones, predicted_tens == labels_tens)).sum().item()\n",
    "\n",
    "      val_accuracy = val_correct / val_total\n",
    "      accuracy = correct / total\n",
    "\n",
    "      print(\n",
    "        f'Epoch {epoch + 1}/{self.num_epochs}, '\n",
    "        f'Loss(Train): {train_loss:.4f}, '\n",
    "        f'Accuracy(Train): {accuracy:.2f}, '\n",
    "        f'Loss(Val): {val_loss:.4f}, '\n",
    "        f'Accuracy(Val): {val_accuracy:.2f}'\n",
    "      )\n",
    "\n",
    "    return train_loss, val_loss, accuracy, val_accuracy\n",
    "\n",
    "  def predict(self, pred_dataset):\n",
    "    pred_loader = DataLoader(pred_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "    self.eval()\n",
    "    predictions = []\n",
    "    labels_true = []\n",
    "\n",
    "    for inputs, (labels_tens, labels_ones) in pred_loader:\n",
    "      outputs_tens, outputs_ones = self(inputs[:, :, :, :32]), self(inputs[:, :, :, 32:])\n",
    "      _, predicted_ones = torch.max(outputs_ones.data, 1)\n",
    "      _, predicted_tens = torch.max(outputs_tens.data, 1)\n",
    "      predictions.append((predicted_tens, predicted_ones))\n",
    "      labels_true.append((labels_tens, labels_ones))\n",
    "    return predictions, labels_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss(Train): 4607.7942, Accuracy(Train): 0.01, Loss(Val): 782.7547, Accuracy(Val): 0.26\n",
      "Epoch 2/10, Loss(Train): 2613.5495, Accuracy(Train): 0.32, Loss(Val): 422.0192, Accuracy(Val): 0.58\n",
      "Epoch 3/10, Loss(Train): 2014.5849, Accuracy(Train): 0.44, Loss(Val): 288.9817, Accuracy(Val): 0.70\n",
      "Epoch 4/10, Loss(Train): 1931.6163, Accuracy(Train): 0.46, Loss(Val): 223.5257, Accuracy(Val): 0.77\n",
      "Epoch 5/10, Loss(Train): 1861.3163, Accuracy(Train): 0.49, Loss(Val): 182.6402, Accuracy(Val): 0.81\n",
      "Epoch 6/10, Loss(Train): 1812.6654, Accuracy(Train): 0.51, Loss(Val): 153.9736, Accuracy(Val): 0.83\n",
      "Epoch 7/10, Loss(Train): 1851.8488, Accuracy(Train): 0.52, Loss(Val): 131.9802, Accuracy(Val): 0.86\n",
      "Epoch 8/10, Loss(Train): 1953.2371, Accuracy(Train): 0.51, Loss(Val): 115.3261, Accuracy(Val): 0.88\n",
      "Epoch 9/10, Loss(Train): 1971.0719, Accuracy(Train): 0.52, Loss(Val): 100.3284, Accuracy(Val): 0.90\n",
      "Epoch 10/10, Loss(Train): 2017.8935, Accuracy(Train): 0.51, Loss(Val): 89.0828, Accuracy(Val): 0.91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2017.8935002088547, 89.08284649252892, 0.513828125, 0.9125625)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_mlp = MultiDigitMLP()\n",
    "\n",
    "multi_mlp.train_model(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75935\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_true = multi_mlp.predict(test_dataset)\n",
    "# for accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(y_pred)):\n",
    "  total += len(y_pred[i][0])\n",
    "  correct += (torch.logical_and(y_pred[i][1] == y_true[i][1], y_pred[i][0] == y_true[i][0])).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1.2: Multi-MNIST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "  def __init__(self, shape):\n",
    "    super(Reshape, self).__init__()\n",
    "    self.shape = shape\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x.view(self.shape)\n",
    "\n",
    "class MultiDigitCNN(nn.Module):\n",
    "  def __init__(self, kernel_size=3, stride=1, padding=1, dropout=0.2, learning_rate=0.001, batch_size=64, num_epochs=5):\n",
    "    super(MultiDigitCNN, self).__init__()\n",
    "\n",
    "    self.num_epochs = num_epochs\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    self.adjust_shape = Reshape((-1, 8192))\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "    self.fc = nn.Linear(8192, 10)\n",
    "\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    # print(x.shape)\n",
    "    x = self.conv1(x)\n",
    "    # print(x.shape)\n",
    "    x = self.relu(x)\n",
    "    x = self.pool(x)\n",
    "    # print(x.shape)\n",
    "    x = self.dropout(x)\n",
    "    x = self.adjust_shape(x)\n",
    "    # print(x.shape)\n",
    "    x = self.fc(x)\n",
    "    # print(x.shape)\n",
    "    return x\n",
    "\n",
    "  def train_model(self, train_data, val_data):\n",
    "    train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    train_loss, val_loss, accuracy, val_accuracy = 0, 0, 0, 0\n",
    "\n",
    "    for epoch in range(self.num_epochs):\n",
    "      self.train()\n",
    "      train_loss = 0.0\n",
    "      correct = 0\n",
    "      total = 0\n",
    "\n",
    "      val_loss = 0.0\n",
    "      val_correct = 0\n",
    "      val_total = 0\n",
    "\n",
    "      for inputs, (labels_tens, labels_ones) in train_loader:\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs_tens, outputs_ones = self(inputs[:, :, :, :32]), self(inputs[:, :, :, 32:])\n",
    "        # print(outputs_tens, outputs_ones)\n",
    "        losses_tens = self.criterion(outputs_tens, labels_tens)\n",
    "        losses_ones = self.criterion(outputs_ones, labels_ones)\n",
    "        total_loss = losses_tens + losses_ones\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        train_loss += total_loss.item()\n",
    "        _, predicted_ones = torch.max(outputs_ones.data, 1)\n",
    "        _, predicted_tens = torch.max(outputs_tens.data, 1)\n",
    "        total += labels_ones.size(0)\n",
    "        correct += (torch.logical_and(predicted_ones == labels_ones, predicted_tens == labels_tens)).sum().item()\n",
    "\n",
    "      self.eval()\n",
    "      for inputs, (labels_tens, labels_ones) in val_loader:\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs_tens, outputs_ones = self(inputs[:, :, :, :32]), self(inputs[:, :, :, 32:])\n",
    "        losses_tens = self.criterion(outputs_tens, labels_tens)\n",
    "        losses_ones = self.criterion(outputs_ones, labels_ones)\n",
    "        total_loss = losses_tens + losses_ones\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        val_loss += total_loss.item()\n",
    "        _, predicted_ones = torch.max(outputs_ones.data, 1)\n",
    "        _, predicted_tens = torch.max(outputs_tens.data, 1)\n",
    "        val_total += labels_ones.size(0)\n",
    "        val_correct += (torch.logical_and(predicted_ones == labels_ones, predicted_tens == labels_tens)).sum().item()\n",
    "        \n",
    "      val_accuracy = val_correct / val_total\n",
    "      accuracy = correct / total\n",
    "\n",
    "      print(\n",
    "        f'Epoch {epoch + 1}/{self.num_epochs}, '\n",
    "        f'Loss(Train): {train_loss:.4f}, '\n",
    "        f'Accuracy(Train): {accuracy:.2f}, '\n",
    "        f'Loss(Val): {val_loss:.4f}, '\n",
    "        f'Accuracy(Val): {val_accuracy:.2f}'\n",
    "      )\n",
    "    return train_loss, val_loss, accuracy, val_accuracy\n",
    "\n",
    "  def predict(self, pred_data):\n",
    "    pred_loader = DataLoader(pred_data, batch_size=self.batch_size, shuffle=False)\n",
    "    self.eval()\n",
    "    predictions = []\n",
    "    labels_true = []\n",
    "\n",
    "    for inputs, (labels_tens, labels_ones) in pred_loader:\n",
    "      outputs_tens, outputs_ones = self(inputs[:, :, :, :32]), self(inputs[:, :, :, 32:])\n",
    "      _, predicted_ones = torch.max(outputs_ones.data, 1)\n",
    "      _, predicted_tens = torch.max(outputs_tens.data, 1)\n",
    "      predictions.append((predicted_tens, predicted_ones))\n",
    "      labels_true.append((labels_tens, labels_ones))\n",
    "    return predictions, labels_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss(Train): 2154.3533, Accuracy(Train): 0.44, Loss(Val): 322.7100, Accuracy(Val): 0.65\n",
      "Epoch 2/5, Loss(Train): 1235.0429, Accuracy(Train): 0.65, Loss(Val): 251.4263, Accuracy(Val): 0.72\n",
      "Epoch 3/5, Loss(Train): 1051.2651, Accuracy(Train): 0.70, Loss(Val): 216.0777, Accuracy(Val): 0.76\n",
      "Epoch 4/5, Loss(Train): 941.0196, Accuracy(Train): 0.73, Loss(Val): 193.7650, Accuracy(Val): 0.78\n",
      "Epoch 5/5, Loss(Train): 867.9561, Accuracy(Train): 0.75, Loss(Val): 177.1804, Accuracy(Val): 0.80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(867.9561473727226, 177.18040171265602, 0.74928125, 0.7995)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_cnn = MultiDigitCNN()\n",
    "\n",
    "multi_cnn.train_model(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76125\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_true = multi_cnn.predict(test_dataset)\n",
    "\n",
    "# for accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(y_pred)):\n",
    "  total += len(y_pred[i][0])\n",
    "  correct += (torch.logical_and(y_pred[i][1] == y_true[i][1], y_pred[i][0] == y_true[i][0])).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1.3: Testing on Single Digit MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# load mnist dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "single_digit_mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, data):\n",
    "    self.data = data\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    image, label = self.data[index]\n",
    "    tens_digit = label // 5\n",
    "    ones_digit = label % 5\n",
    "    return image, (tens_digit, ones_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_double_digit_mnist = CustomDataset(single_digit_mnist_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on multi mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009183333333333333\n"
     ]
    }
   ],
   "source": [
    "y_pred_mlp, y_true_mlp = multi_mlp.predict(pseudo_double_digit_mnist)\n",
    "\n",
    "# for accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(y_pred_mlp)):\n",
    "  total += len(y_pred_mlp[i][0])\n",
    "  correct += (5*y_pred_mlp[i][0] + y_pred_mlp[i][1] == 5*y_true_mlp[i][0] + y_true_mlp[i][1]).sum().item()\n",
    "  \n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on multi cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012816666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred_cnn, y_true_cnn = multi_cnn.predict(pseudo_double_digit_mnist)\n",
    "\n",
    "# for accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(y_pred_cnn)):\n",
    "  total += len(y_pred_cnn[i][0])\n",
    "  correct += (y_pred_cnn[i][1] + 5*y_pred_cnn[i][0] == y_true_cnn[i][1] + 5*y_true_cnn[i][0]).sum().item()\n",
    "  \n",
    "print(correct / total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
